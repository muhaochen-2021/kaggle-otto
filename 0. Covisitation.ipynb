{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00373,
     "end_time": "2022-11-10T16:03:20.9748",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.97107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1 - Candidate Generation with RAPIDS\n",
    "For candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T16:18:32.946787Z",
     "iopub.status.busy": "2023-01-01T16:18:32.946256Z",
     "iopub.status.idle": "2023-01-01T16:18:35.435361Z",
     "shell.execute_reply": "2023-01-01T16:18:35.433396Z",
     "shell.execute_reply.started": "2023-01-01T16:18:32.946639Z"
    },
    "papermill": {
     "duration": 3.036143,
     "end_time": "2022-11-10T16:03:24.014816",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.978673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00424,
     "end_time": "2022-11-10T16:03:24.023816",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.019576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Three Co-visitation Matrices with RAPIDS\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T16:18:35.438130Z",
     "iopub.status.busy": "2023-01-01T16:18:35.437489Z",
     "iopub.status.idle": "2023-01-01T16:18:35.443564Z",
     "shell.execute_reply": "2023-01-01T16:18:35.442604Z",
     "shell.execute_reply.started": "2023-01-01T16:18:35.438092Z"
    }
   },
   "outputs": [],
   "source": [
    "MODE = \"local\" # \"kaggle\"\n",
    "\n",
    "if MODE == \"kaggle\":\n",
    "    readpath = '../raw_data/*_parquet/*'\n",
    "elif MODE == \"local\":\n",
    "    readpath = '../raw_data/deotte_radek/*_parquet/*'\n",
    "    \n",
    "files = glob.glob(readpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-01T16:18:35.445917Z",
     "iopub.status.busy": "2023-01-01T16:18:35.445173Z",
     "iopub.status.idle": "2023-01-01T16:18:52.878776Z",
     "shell.execute_reply": "2023-01-01T16:18:52.877737Z",
     "shell.execute_reply.started": "2023-01-01T16:18:35.445878Z"
    },
    "papermill": {
     "duration": 0.063943,
     "end_time": "2022-11-10T16:03:24.091816",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.027873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32') # convert to seconds\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8') # convert to int\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004089,
     "end_time": "2022-11-10T16:03:24.100502",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.096413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted\n",
    "- df保留 CLICKS、CARTS 和 ORDERS\n",
    "- 行为时间小于1days\n",
    "- 删除同一个session下，重复的'aid_x', 'aid_y', 'type_y'\n",
    "- weights: 行为权重 {0:1, 1:5, 2:4}\n",
    "- 保留前50个items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T16:18:52.880915Z",
     "iopub.status.busy": "2023-01-01T16:18:52.880496Z",
     "iopub.status.idle": "2023-01-01T16:18:58.554388Z",
     "shell.execute_reply": "2023-01-01T16:18:58.552658Z",
     "shell.execute_reply.started": "2023-01-01T16:18:52.880879Z"
    },
    "papermill": {
     "duration": 566.561189,
     "end_time": "2022-11-10T16:12:50.666123",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.104934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "type_weight = {0:1, 1:5, 2:4}\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 16\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# pieces循环\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # chunk循环\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # READ_CT 循环\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE， 5个\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)  # 合并5个df\n",
    "            # 按照 session 升序, ts 降序\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # 每个人最新的30个items\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount() # session 累积计数\n",
    "            df = df.loc[df.n<30].drop('n',axis=1) # 只保留前30个session\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            # 同一个session的 item-pair\n",
    "            # session, aid_x, ts_x, type_x, aid_y, ts_y, type_y\n",
    "            df = df.merge(df,on='session')\n",
    "            # 行为时间小于1天 & item-pair中的item不同\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # pieces循环数据切分\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]  # 内存管理\n",
    "            \n",
    "            # 删除同一用户的重复的 'aid_x', 'aid_y', 'type_y'\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight) # 0:1, 1:5, 2:4\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "\n",
    "    # 'aid_x','aid_y' 匹配的所有权值累加        \n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    # 按照 aid_ 升序, wgt 降序\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # 每个 aid 保留wgt更大的items前50个.\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount() # 根据wgt排序\n",
    "    tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'{MODE}_top_50_carts_orders_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03219,
     "end_time": "2022-11-10T16:12:50.730634",
     "exception": false,
     "start_time": "2022-11-10T16:12:50.698444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix\n",
    "- df只保留 CARTS 和 ORDERS\n",
    "- 行为时间小于14days\n",
    "- 删除同一个session下，重复的'aid_x', 'aid_y', 'type_y'\n",
    "- weights: CART:1, ORDERS:1\n",
    "- 保留50个items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2023-01-01T16:18:58.555695Z",
     "iopub.status.idle": "2023-01-01T16:18:58.556405Z",
     "shell.execute_reply": "2023-01-01T16:18:58.556172Z",
     "shell.execute_reply.started": "2023-01-01T16:18:58.556138Z"
    },
    "papermill": {
     "duration": 113.735315,
     "end_time": "2022-11-10T16:14:44.498182",
     "exception": false,
     "start_time": "2022-11-10T16:12:50.762867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 1\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# pieces循环\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # chunk循环\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # read_ct循环\n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE， 5个\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)  # 合并5个df\n",
    "            df = df.loc[df['type'].isin([1,2])] # 只保留 CARTS 和 ORDERS\n",
    "            # 按照 session 升序, ts 降序\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            df = df.reset_index(drop=True)\n",
    "            # 每个人最新的30个items\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            # 同一个session的 item-pair\n",
    "            # session, aid_x, ts_x, type_x, aid_y, ts_y, type_y\n",
    "            df = df.merge(df,on='session')\n",
    "            # 行为时间小于14days & item-pair中的item不同\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            \n",
    "            # pieces循环数据切分\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # 删除同一用户的重复的 'aid_x', 'aid_y','type_y'\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = 1 # CART 和 ORDERS 权重相同\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "    # 'aid_x','aid_y' 匹配的所有权值的累加\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "\n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    # 按照 aid_ 升序, wgt 降序\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # 每个aid_x保留匹配度最高的前50个items\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount() # 根据wgt排序\n",
    "    tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'{MODE}_top_50_buy2buy_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04526,
     "end_time": "2022-11-10T16:14:44.58589",
     "exception": false,
     "start_time": "2022-11-10T16:14:44.54063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted\n",
    "- df保留 CLICKS、CARTS 和 ORDERS\n",
    "- 行为时间小于1days\n",
    "- 删除同一个session下，重复的'aid_x', 'aid_y'\n",
    "- weights: 时间戳越大权重越大\n",
    "- 保留50个items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2023-01-01T16:18:58.557805Z",
     "iopub.status.idle": "2023-01-01T16:18:58.558521Z",
     "shell.execute_reply": "2023-01-01T16:18:58.558272Z",
     "shell.execute_reply.started": "2023-01-01T16:18:58.558249Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2022-11-10T16:14:44.629032",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# pieces循环\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # chunk循环\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # read_ct循环\n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE， 5个\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0) # 合并5个df\n",
    "            # 按照 session 升序, ts 降序\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            df = df.reset_index(drop=True)\n",
    "            # 每个人最新的30个items\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            # 同一个session的 item-pair\n",
    "            # session, aid_x, ts_x, type_x, aid_y, ts_y, type_y\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # pieces循环数据切分\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)] # 内存管理\n",
    "            \n",
    "            # 删除同一用户的重复的 'aid_x', 'aid_y'\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            # 时间戳作为权重，时间戳越大权重越大\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            # 1659304800 : min ts\n",
    "            # 1662328791 : max ts\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "    # 'aid_x','aid_y' 匹配的所有权值累加\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    # 按照 aid_ 升序, wgt 降序\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # 每个aid_x 保留50个wgt更大的items.\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount() # 根据wgt排序\n",
    "    tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'{MODE}_top_50_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-01T16:18:58.559924Z",
     "iopub.status.idle": "2023-01-01T16:18:58.560745Z",
     "shell.execute_reply": "2023-01-01T16:18:58.560463Z",
     "shell.execute_reply.started": "2023-01-01T16:18:58.560435Z"
    }
   },
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b79a61544c9a744d09395b396d14bdc3ab2980641b64ddb1c7bc6d7b892900a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
